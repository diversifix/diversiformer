{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import read_wiki_sents\n",
    "\n",
    "sents = read_wiki_sents()\n",
    "sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train = list(load_dataset(\"diversifix/inclusive_words\")[\"train\"])\n",
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "for row in train:\n",
    "    if not row[\"exclusive\"] in data.keys():\n",
    "        data[row[\"exclusive\"]] = []\n",
    "    data[row[\"exclusive\"]].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\", \"attribute_ruler\", \"parser\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "memory = Memory(\"~/.cache\", verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def get_docs(n):\n",
    "    return list(nlp.pipe(sents[:n], batch_size=1000, n_process=4))\n",
    "\n",
    "docs = get_docs(50_000)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(93020)\n",
    "\n",
    "people_sents = []\n",
    "for doc in docs:\n",
    "    matches = [t for t in doc if t.pos_ == \"NOUN\" and t.lemma_ in data.keys()]\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    random.shuffle(matches)\n",
    "    t = matches[0]\n",
    "    alternatives = data[t.lemma_]\n",
    "    if t.morph.get(\"Number\") == [\"Sing\"]:\n",
    "        alternatives = [a for a in alternatives if a[\"applicable\"] in [\"in_singular\", \"always\"]]\n",
    "    if len(alternatives) == 0:\n",
    "        continue\n",
    "    random.shuffle(alternatives)\n",
    "    alt = alternatives[0]\n",
    "    post = \" oder \" + t.lemma_ if alt[\"gender_of_inclusive\"] == \"female\" and random.random() > 0.5 else \"\"\n",
    "    inclusive = alt[\"inclusive\"] + post\n",
    "    people_sents.append((doc.text, t.lemma_, inclusive))\n",
    "(len(people_sents), people_sents[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import chunks\n",
    "\n",
    "batches = []\n",
    "for chunk in chunks(people_sents, 8):\n",
    "    inputs = [\"\"\"(1)\\nOriginal: \"Die Schüler kamen zu spät.\"\\nMit Ersetzung: \"Die Schülerinnen und Schüler kamen zu spät.\"\\n\\n(2)\\nOriginal: \"Sie werden dem neuen Kanzler gratulieren.\"\\nMit Ersetzung: \"Sie werden der neuen Kanzlerin oder dem neuen Kanzler gratulieren.\"\\n\"\"\"]\n",
    "    instructions = [\"\"\"Führe die folgenden Ersetzungen durch. Verändere den Satz und die Ersatzwörter dazu gegebenenfalls grammatisch, sodass ein grammatisch korrekter und flüssiger Satz entsteht.\\n\\n(1) Ersetze \"Schüler\" durch eine entsprechend angepasste Form von \"Schülerin oder Schüler\".\\n(2) Ersetze \"Kanzler\" durch eine entsprechend angepasste Form von \"Kanzlerin oder Kanzler\".\"\"\"]\n",
    "    for i, (sent, a, b) in enumerate(chunk, 3):\n",
    "        inputs.append(f\"\"\"({i})\\nOriginal: \"{sent}\"\\nMit Ersetzung: ___\\n\"\"\")\n",
    "        instructions.append(f\"\"\"({i}) Ersetze \"{a}\" durch eine entsprechend angepasste Form von \"{b}\".\"\"\")\n",
    "    batches.append((chunk, \"\\n\".join(inputs), \"\\n\".join(instructions)))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_replacements(input, instruction):\n",
    "    response = openai.Edit.create(\n",
    "        engine=\"text-davinci-edit-001\",\n",
    "        input=input,\n",
    "        instruction=instruction,\n",
    "        temperature=0,\n",
    "        top_p=1\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def get_unfiltered_training_data(batch, i):\n",
    "    print(i)\n",
    "    chunk, input, instruction = batch\n",
    "    try:\n",
    "        output = get_replacements(input, instruction)\n",
    "    except:\n",
    "        print(batch)\n",
    "        return []\n",
    "    replacements = re.findall(r\"Mit Ersetzung: \\\"(.*)\\\"\", output)[2:]\n",
    "    return [(sent, a, b, rep) for (sent, a, b), rep in zip(chunk, replacements)]\n",
    "\n",
    "\n",
    "utd = Parallel(n_jobs=2)(\n",
    "    [delayed(get_unfiltered_training_data)(batch, i) for i, batch in enumerate(batches[:1200])]\n",
    ")\n",
    "unfiltered_training_data = list(chain(*utd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "@memory.cache\n",
    "def filter_data(data, i):\n",
    "    print(i)\n",
    "    out_data = []\n",
    "    for sent, a, b, rep in data:\n",
    "        r = requests.post(\n",
    "            \"http://localhost:8081/v2/check\",\n",
    "            data={\"text\": rep, \"language\": \"de-DE\", \"enabledCategories\": \"PUNCTUATION,CASING,COLLOCATIONS,CONFUSED_WORDS,CREATIVE_WRITING,GRAMMAR,MISC,MISUSED_TERMS_EU_PUBLICATIONS,NONSTANDARD_PHRASES,REDUNDANCY,SEMANTICS,TEXT_ANALYSIS,STYLE\", \"disabledCategories\": \"TYPOS,TYPOGRAPHY\"},\n",
    "            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n",
    "        )\n",
    "        matches = r.json()[\"matches\"]\n",
    "        if len(matches) == 0:\n",
    "            out_data.append(dict(x=sent, a=a, b=b, y=rep))\n",
    "        else:\n",
    "            print(rep)\n",
    "    return out_data\n",
    "\n",
    "training_data = list(chain(*Parallel(n_jobs=1)(delayed(filter_data)(data, i) for i, data in enumerate(list(chunks(unfiltered_training_data, 100))))))\n",
    "(len(unfiltered_training_data), len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/training_data_gender.json\", \"w\") as f:\n",
    "    json.dump(training_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "with jsonlines.open(\"../data/training_data_gender.jsonl\", mode=\"w\") as writer:\n",
    "    writer.write_all(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
