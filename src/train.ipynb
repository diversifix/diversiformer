{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will follow [Huggingface's translation tutorial](https://huggingface.co/docs/transformers/tasks/translation) more or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_EPOCH=0\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff for running the same notebook locally and on Google Colab for training\n",
    "import sys\n",
    "COLAB_PATH = \"/content/drive/MyDrive/Colab Notebooks/diversiformer/\"\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "fp = COLAB_PATH + \"training_data_gender.jsonl\" if IN_COLAB else \"../data/training_data_gender.jsonl\"\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    %pip install transformers datasets sacrebleu sentencepiece carbontracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=fp)\n",
    "data = data[\"train\"].train_test_split(test_size=0.2, shuffle=False)\n",
    "data[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    inputs = [f\"\"\"Ersetze \"{a}\" durch \"{b}\": {x}\"\"\" for a, b, x in zip(data[\"a\"], data[\"b\"], data[\"x\"])]\n",
    "    print(inputs)\n",
    "    targets = data[\"y\"]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFT5ForConditionalGeneration\n",
    "\n",
    "try:\n",
    "    # load model\n",
    "    model = TFT5ForConditionalGeneration.from_pretrained((COLAB_PATH if IN_COLAB else \"../data/\") + f\"checkpoint_{START_EPOCH}_epochs\")\n",
    "except:\n",
    "    print(\"WARNING: Could not load local model.\")\n",
    "    model = TFT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = tokenized_data[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = tokenized_data[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "Ersetze \"Autofahrer\" durch \"Mensch, der Auto fährt\": Heute morgen im Stau haben mich die Autofahrer wieder sehr aufgeregt.\n",
    "Ersetze \"Behinderter\" durch  \"Mensch mit Behinderungen\": Behinderte mit entsprechenden Ausweisen bekommen ermäßigten Eintritt.\n",
    "Ersetze \"Student\" durch \"studierende Person\" bzw. \"Studierende\": Viele faule Studenten studieren gar nicht wirklich.\n",
    "Ersetze \"Student\" durch \"studierende Person\" bzw. \"Studierende\": Maria ist kein Student.\n",
    "Ersetze \"Lehrer\" durch \"Lehrerin oder Lehrer\" bzw. \"Kollegium\": Die Lehrer machen morgen einen Ausflug.\n",
    "Ersetze \"Lehrer\" durch \"Lehrerin oder Lehrer\" bzw. \"Kollegium\": Ein promovierter Mathelehrer ist noch nie im Unterricht eingeschlafen.\n",
    "Ersetze \"Polizist\" durch \"Polizistin oder Polizist\": Die Polizisten machen oft Überstunden.\n",
    "Ersetze \"Gaul\" durch \"Stute oder Gaul\": Einem geschenkten Gaul schaut man nicht ins Maul.\n",
    "\"\"\".strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import json\n",
    "    from carbontracker.tracker import CarbonTracker\n",
    "    from transformers import pipeline\n",
    "\n",
    "    tracker = CarbonTracker(epochs=EPOCHS, verbose=2)\n",
    "    try:\n",
    "        with open(COLAB_PATH + \"example_predictions.json\") as f:\n",
    "            example_eval = json.load(f)\n",
    "    except:\n",
    "        example_eval = []\n",
    "    example_eval = []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"Epoch {START_EPOCH + epoch}\")\n",
    "        tracker.epoch_start()\n",
    "        model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=1)\n",
    "        tracker.epoch_end()\n",
    "        generator = pipeline(\n",
    "            task=\"text2text-generation\", model=model, tokenizer=tokenizer\n",
    "        )\n",
    "        example_eval.append(\n",
    "            [\n",
    "                dict(\n",
    "                    epoch=START_EPOCH + epoch,\n",
    "                    prompt=prompt,\n",
    "                    response=generator(prompt)[0][\"generated_text\"],\n",
    "                )\n",
    "                for prompt in examples\n",
    "            ]\n",
    "        )\n",
    "        with open(COLAB_PATH + \"example_predictions.json\", \"w\") as f:\n",
    "            json.dump(example_eval, f, indent=2, ensure_ascii=False)\n",
    "    tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    model.save_pretrained(COLAB_PATH + f\"checkpoint_{START_EPOCH+EPOCHS}_epochs\")\n",
    "    model.save(COLAB_PATH + f\"tf_checkpoint_{START_EPOCH+EPOCHS}_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = list(zip(data[\"test\"][\"x\"], data[\"test\"][\"a\"], data[\"test\"][\"b\"], data[\"test\"][\"y\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, a, b, y in entries[:5]:\n",
    "    prompt = f\"\"\"Ersetze \"{a}\" durch \"{b}\": {x}\"\"\"\n",
    "    print(prompt)\n",
    "    prediction = generator(prompt)[0][\"generated_text\"]\n",
    "    print(prediction)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
