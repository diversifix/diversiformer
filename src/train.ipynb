{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will follow [Huggingface's translation tutorial](https://huggingface.co/docs/transformers/tasks/translation) more or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff for running the same notebook locally and on Google Colab for training\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "fp = \"/content/drive/MyDrive/Colab Notebooks/ersetzBERT/training_data_gender.jsonl\" if IN_COLAB else \"../data/training_data_gender.jsonl\"\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    %pip install transformers datasets sacrebleu sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=fp)\n",
    "data = data[\"train\"].train_test_split(test_size=0.2, shuffle=False)\n",
    "data[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    inputs = [f\"\"\"Ersetze \"{a}\" durch \"{b}\": {x}\"\"\" for a, b, x in zip(data[\"a\"], data[\"b\"], data[\"x\"])]\n",
    "    print(inputs)\n",
    "    targets = data[\"y\"]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFT5ForConditionalGeneration\n",
    "\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = tokenized_data[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = tokenized_data[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %reload_ext tensorboard\n",
    "    log_folder = \"logs\"\n",
    "    %tensorboard --logdir={log_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "    history = model.fit(\n",
    "        x=tf_train_set, validation_data=tf_test_set, epochs=20, callbacks=[TensorBoard(log_dir=log_folder)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    model.save(\"/content/drive/MyDrive/Colab Notebooks/ersetzBERT/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = list(zip(data[\"test\"][\"x\"], data[\"test\"][\"a\"], data[\"test\"][\"b\"], data[\"test\"][\"y\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, a, b, y in entries[:5]:\n",
    "    prompt = f\"\"\"Ersetze \"{a}\" durch \"{b}\": {x}\"\"\"\n",
    "    print(prompt)\n",
    "    prediction = generator(prompt)[0][\"generated_text\"]\n",
    "    print(prediction)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "Ersetze \"Autofahrer\" durch \"Mensch, der Auto fährt\": Heute morgen im Stau haben mich die Autofahrer wieder sehr aufgeregt.\n",
    "Ersetze \"Behinderter\" durch  \"Mensch mit Behinderungen\": Behinderte mit entsprechenden Ausweisen bekommen ermäßigten Eintritt.\n",
    "Ersetze \"Student\" durch \"studierende Person\" bzw. \"Studierende\": Viele faule Studenten studieren gar nicht wirklich.\n",
    "Ersetze \"Student\" durch \"studierende Person\" bzw. \"Studierende\": Maria ist kein Student.\n",
    "Ersetze \"Lehrer\" durch \"Lehrerin oder Lehrer\" bzw. \"Kollegium\": Die Lehrer machen morgen einen Ausflug.\n",
    "Ersetze \"Lehrer\" durch \"Lehrerin oder Lehrer\" bzw. \"Kollegium\": Ein promovierter Mathelehrer ist noch nie im Unterricht eingeschlafen.\n",
    "Ersetze \"Polizist\" durch \"Polizistin oder Polizist\": Die Polizisten machen oft Überstunden.\n",
    "Ersetze \"Gaul\" durch \"Stute oder Gaul\": Einem geschenkten Gaul schaut man nicht ins Maul.\n",
    "\"\"\".strip().split(\"/n\")\n",
    "for prompt in examples:\n",
    "    print(prompt)\n",
    "    prediction = generator(prompt)[0][\"generated_text\"]\n",
    "    print(prediction)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
